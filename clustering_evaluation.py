import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import geohash
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Qt5Agg')
import os
import time


class FoursquareDataLoader:
    def __init__(self, data_path):
        self.data_path = data_path

    def load_raw_data(self):
        """åŠ è½½åŸå§‹Foursquareæ•°æ®é›†"""
        columns = ['user_id', 'poi_id', 'poi_category_id', 'poi_category_name',
                   'latitude', 'longitude', 'timezone_offset', 'timestamp']

        try:
            # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']

            for encoding in encodings:
                try:
                    df = pd.read_csv(self.data_path, sep='\t', header=None,
                                     names=columns, encoding=encoding,
                                     on_bad_lines='skip')
                    print(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {os.path.basename(self.data_path)} (ç¼–ç : {encoding})")
                    print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡ç­¾åˆ°è®°å½•")
                    return df
                except UnicodeDecodeError:
                    continue

            # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œè¿›è¡Œé”™è¯¯å¤„ç†
            df = pd.read_csv(self.data_path, sep='\t', header=None,
                             names=columns, encoding='latin-1',
                             on_bad_lines='skip',
                             engine='python')
            print(f"ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆåŠ è½½æ•°æ®é›†: {os.path.basename(self.data_path)}")
            print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡ç­¾åˆ°è®°å½•")
            return df

        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None

    def preprocess_data(self, df, min_user_checkins=10, min_poi_checkins=10):
        """æ•°æ®é¢„å¤„ç†"""
        print("\nå¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # 1. åˆ é™¤ä½é¢‘ç”¨æˆ·
        user_checkin_counts = df['user_id'].value_counts()
        valid_users = user_checkin_counts[user_checkin_counts >= min_user_checkins].index
        df = df[df['user_id'].isin(valid_users)]
        print(f"è¿‡æ»¤ä½é¢‘ç”¨æˆ·å: {len(df)} æ¡è®°å½•")

        # 2. åˆ é™¤ä½é¢‘POI
        poi_checkin_counts = df['poi_id'].value_counts()
        valid_pois = poi_checkin_counts[poi_checkin_counts >= min_poi_checkins].index
        df = df[df['poi_id'].isin(valid_pois)]
        print(f"è¿‡æ»¤ä½é¢‘POIå: {len(df)} æ¡è®°å½•")

        # 3. è½¬æ¢æ—¶é—´æˆ³
        sample_timestamps = df['timestamp'].head(3).tolist()
        print(f"æ—¶é—´æˆ³æ ·ä¾‹: {sample_timestamps}")

        try:
            df['timestamp'] = pd.to_datetime(df['timestamp'], format='%a %b %d %H:%M:%S %z %Y')
        except Exception as e:
            print(f"æ ‡å‡†æ ¼å¼å¤±è´¥: {e}")
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            invalid_count = df['timestamp'].isna().sum()
            if invalid_count > 0:
                # å°†æ—¶é—´æˆ³è½¬æ¢å¤±è´¥çš„è®°å½•åˆ é™¤"
                df = df.dropna(subset=['timestamp'])

        df = df.sort_values(['user_id', 'timestamp'])

        # 4. å»é™¤é‡å¤POIä¿¡æ¯ï¼ˆç”¨äºèšç±»ï¼‰
        pois_df = df[['poi_id', 'latitude', 'longitude', 'poi_category_id']].drop_duplicates('poi_id')
        print(f"å»é‡å: {len(pois_df)}")

        return df, pois_df

    def get_pois_for_clustering(self, pois_df):
        """è·å–ç”¨äºèšç±»çš„POIæ•°æ®"""
        clustering_data = pois_df[['poi_id', 'latitude', 'longitude']].copy()
        return clustering_data


class ClusteringEvaluator:
    def __init__(self, pois_data):
        """
        pois_data: DataFrame with columns ['poi_id', 'latitude', 'longitude']
        """
        self.pois_data = pois_data
        self.coords = pois_data[['latitude', 'longitude']].values

    def _time_method(self, method_func, *args, **kwargs):
        start_time = time.time()
        result = method_func(*args, **kwargs)
        end_time = time.time()
        elapsed_time = end_time - start_time
        return result, elapsed_time
    def original_dbscan(self, eps_km, min_samples):
        print("=" * 20+"åŸå§‹DBSCANèšç±»"+"=" * 20)
        start_time = time.time()
        coords_rad = np.radians(self.coords)   # å°†ç»çº¬åº¦è½¬æ¢ä¸ºå¼§åº¦
        eps_rad = eps_km / 6371.0  # å°†kmè½¬æ¢ä¸ºå¼§åº¦
        db = DBSCAN(eps=eps_rad, min_samples=min_samples, metric='haversine')
        labels = db.fit_predict(coords_rad)
        end_time = time.time()
        elapsed_time = end_time - start_time

        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        n_total_points = len(labels)
        print(f"ğŸ“Š èšç±»ç»“æœç»Ÿè®¡:")
        print(f"  ç°‡æ•°é‡: {n_clusters}ä¸ª")
        print(f"  å™ªå£°ç‚¹: {n_noise}ä¸ª ({n_noise / n_total_points * 100:.1f}%)")
        print(f"  æ€»ç‚¹æ•°: {n_total_points}ä¸ª")
        print(f"  å¹³å‡ç°‡å¤§å°: {n_total_points / n_clusters:.1f}ç‚¹/ç°‡" if n_clusters > 0 else "  å¹³å‡ç°‡å¤§å°: N/A")
        print(f"  æ‰§è¡Œæ—¶é—´: {elapsed_time:.3f}ç§’")
        return labels, elapsed_time

    def geohash_dbscan(self, eps_km, min_samples, geohash_precision):
        """GeoHash + DBSCANæ–¹æ³• """
        print("=" * 20 + "GeoHash + DBSCANæ–¹æ³•" + "=" * 20)
        start_time = time.time()
        pois_data_reset = self.pois_data.reset_index(drop=True)  # é‡ç½®ç´¢å¼•ç¡®ä¿è¿ç»­æ€§
        coords_rad_global = np.radians(self.coords)  # å°†å…¨å±€åæ ‡è½¬æ¢ä¸ºå¼§åº¦
        geohash_to_pois = defaultdict(list)  # GeoHashç¼–ç åˆ†ç»„

        for idx, row in pois_data_reset.iterrows():
            gh = geohash.encode(row['latitude'], row['longitude'], geohash_precision)
            geohash_to_pois[gh].append((idx, row['latitude'], row['longitude']))

        print(f"GeoHashåˆ’åˆ†åŒºåŸŸæ•°é‡: {len(geohash_to_pois)}")
        all_labels = np.full(len(pois_data_reset), -1)  # åˆå§‹åŒ–æ ‡ç­¾æ•°ç»„
        cluster_id_offset = 0

        eps_rad = eps_km / 6371.0  # å°†kmè½¬æ¢ä¸ºå¼§åº¦
        region_stats = []
        processed_points = 0
        total_clusters_created = 0

        # åŒºåŸŸå¤„ç†
        for i, (gh, pois_in_gh) in enumerate(geohash_to_pois.items()):
            n_points = len(pois_in_gh)

            if n_points < min_samples:
                region_stats.append((gh, n_points, 0, n_points))
                continue

            indices = [item[0] for item in pois_in_gh]  # æå–è¯¥åŒºåŸŸçš„åæ ‡ï¼ˆå¼§åº¦ï¼‰
            coords_gh_rad = coords_rad_global[indices]  # ä½¿ç”¨å…¨å±€çš„å¼§åº¦åæ ‡

            # åœ¨è¯¥åŒºåŸŸå†…è¿è¡ŒDBSCANï¼ˆHaversineè·ç¦»ï¼‰
            db = DBSCAN(eps=eps_rad, min_samples=min_samples, metric='haversine')
            labels_gh = db.fit_predict(coords_gh_rad)

            # ç»Ÿè®¡è¯¥åŒºåŸŸèšç±»æƒ…å†µ
            unique_labels = set(labels_gh)
            n_clusters = len([l for l in unique_labels if l != -1])
            n_noise = np.sum(labels_gh == -1)
            region_stats.append((gh, n_points, n_clusters, n_noise))
            total_clusters_created += n_clusters
            processed_points += n_points

            # é‡æ–°ç¼–å·å¹¶èµ‹å€¼æ ‡ç­¾
            for j, label in enumerate(labels_gh):
                idx = indices[j]
                if label != -1:
                    all_labels[idx] = label + cluster_id_offset

            # æ›´æ–°åç§»é‡
            if n_clusters > 0:
                max_label = max([l for l in labels_gh if l != -1])
                cluster_id_offset += (max_label + 1)

        end_time = time.time()
        elapsed_time = end_time - start_time
        n_total_points = len(pois_data_reset)
        n_total_noise = np.sum(all_labels == -1)


        print(f"\n--æ€»ä½“èšç±»ç»Ÿè®¡:")
        print(f"  ç°‡æ•°é‡: {total_clusters_created}ä¸ª")
        print(f"  å™ªå£°ç‚¹: {n_total_noise}ä¸ª ({n_total_noise / n_total_points * 100:.1f}%)")
        print(f"  æ€»ç‚¹æ•°: {n_total_points}ä¸ª")
        print(
            f"  å¹³å‡ç°‡å¤§å°: {processed_points / total_clusters_created:.1f}ç‚¹/ç°‡" if total_clusters_created > 0 else "  å¹³å‡ç°‡å¤§å°: N/A")
        print(f"  æ‰§è¡Œæ—¶é—´: {elapsed_time:.3f}ç§’")

        # åˆ†æåŒºåŸŸå¤§å°åˆ†å¸ƒ
        region_sizes = [s[1] for s in region_stats]
        print(f"\n--åŒºåŸŸå¤§å°åˆ†å¸ƒ:")
        print(f"  æœ€å¤§åŒºåŸŸ: {max(region_sizes)}ç‚¹")
        print(f"  æœ€å°åŒºåŸŸ: {min(region_sizes)}ç‚¹")
        print(f"  å¹³å‡åŒºåŸŸ: {np.mean(region_sizes):.1f}ç‚¹")
        print(f"  ä¸­ä½æ•°åŒºåŸŸ: {np.median(region_sizes):.1f}ç‚¹")

        valid_regions = [s for s in region_stats if s[1] >= min_samples]
        print(f"æœ‰æ•ˆåŒºåŸŸæ•°é‡: {len(valid_regions)}/{len(region_stats)}")

        return all_labels,elapsed_time

    def evaluate_clustering(self, labels, method_name):
        """è¯„ä¼°èšç±»è´¨é‡"""
        # è¿‡æ»¤æ‰å™ªå£°ç‚¹ï¼ˆlabel = -1ï¼‰
        valid_mask = labels != -1
        valid_labels = labels[valid_mask]
        valid_coords = self.coords[valid_mask]

        if len(set(valid_labels)) < 2:
            print(f"{method_name}: æœ‰æ•ˆèšç±»æ•°å°‘äº2ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
            return {
                'method': method_name,
                'silhouette_score': np.nan,
                'calinski_harabasz_score': np.nan,
                'davies_bouldin_score': np.nan,
                'n_clusters': len(set(valid_labels)),
                'n_noise': np.sum(labels == -1),
                'coverage': len(valid_labels) / len(labels),
                'avg_cluster_size': 0
            }

        # è®¡ç®—å¹³å‡ç°‡å¤§å°
        cluster_sizes = [np.sum(valid_labels == label) for label in set(valid_labels)]
        avg_cluster_size = np.mean(cluster_sizes)

        metrics = {
            'method': method_name,
            'silhouette_score': silhouette_score(valid_coords, valid_labels),
            'calinski_harabasz_score': calinski_harabasz_score(valid_coords, valid_labels),
            'davies_bouldin_score': davies_bouldin_score(valid_coords, valid_labels),
            'n_clusters': len(set(valid_labels)),
            'n_noise': np.sum(labels == -1),
            'coverage': len(valid_labels) / len(labels),
            'avg_cluster_size': avg_cluster_size
        }
        return metrics

    def compare_methods(self, eps_km=1.0, min_samples=5, geohash_precision=6):
        """æ¯”è¾ƒä¸¤ç§èšç±»æ–¹æ³• - ä½¿ç”¨kmå•ä½"""
        print("\nèšç±»è´¨é‡è¯„ä¼°...")

        # æ–¹æ³•1: åŸå§‹DBSCAN with Haversine
        labels_original, time_original = self.original_dbscan(eps_km, min_samples)
        metrics_original = self.evaluate_clustering(labels_original, "Original DBSCAN")
        metrics_original['execution_time'] = time_original

        # æ–¹æ³•2: GeoHash + DBSCAN with Haversine
        labels_geohash, time_geohash = self.geohash_dbscan(eps_km, min_samples, geohash_precision)
        metrics_geohash = self.evaluate_clustering(labels_geohash, "GeoHash+DBSCAN")
        metrics_geohash['execution_time'] = time_geohash

        # æ·»åŠ results_dfçš„å®šä¹‰
        results_df = pd.DataFrame([metrics_original, metrics_geohash])

        # ç®€å•çš„æ€§èƒ½å¯¹æ¯”è¾“å‡º
        print("=" * 20 + "æ€§èƒ½å¯¹æ¯”æ€»ç»“" + "=" * 20)
        print(f"åŸå§‹DBSCANæ‰§è¡Œæ—¶é—´: {time_original:.3f}ç§’")
        print(f"GeoHash+DBSCANæ‰§è¡Œæ—¶é—´: {time_geohash:.3f}ç§’")
        print(f"é€Ÿåº¦æå‡: {time_original / time_geohash:.2f}å€")

        return results_df, labels_original, labels_geohash

    def visualize_comparison(self, labels_original, labels_geohash, save_path=None):
        """å¯è§†åŒ–å¯¹æ¯”ä¸¤ç§æ–¹æ³•"""
        try:
            fig, axes = plt.subplots(1, 2, figsize=(16, 6))

            # åŸå§‹DBSCANç»“æœ
            scatter1 = axes[0].scatter(self.coords[:, 1],self.coords[:, 0],
                                       c=labels_original, cmap='tab10', s=8, alpha=0.7)
            axes[0].set_title('Original DBSCAN Clustering', fontsize=14)
            axes[0].set_xlabel('Longitude')
            axes[0].set_ylabel('Latitude')
            plt.colorbar(scatter1, ax=axes[0])

            # GeoHash+DBSCANç»“æœ
            scatter2 = axes[1].scatter(self.coords[:, 1],self.coords[:, 0],
                                       c=labels_geohash, cmap='tab10', s=8, alpha=0.7)
            axes[1].set_title('GeoHash + DBSCAN Clustering', fontsize=14)
            axes[1].set_xlabel('Longitude')
            axes[1].set_ylabel('Latitude')
            plt.colorbar(scatter2, ax=axes[1])

            plt.tight_layout()

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {save_path}")

            plt.show()

        except Exception as e:
            print(f"å¯è§†åŒ–å¤±è´¥: {e}")
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {save_path} (ä½†æ— æ³•æ˜¾ç¤º)")
            plt.close()


def main():
    data_path = r"E:\code\lwCode\My_Code\dataset\dataset_TSMC2014_NYC.txt"  # æ•°æ®é›†è·¯å¾„

    # 1. åŠ è½½æ•°æ®
    loader = FoursquareDataLoader(data_path)
    raw_df = loader.load_raw_data()

    if raw_df is None:
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œç¼–ç ")
        return

    # 2. é¢„å¤„ç†æ•°æ®
    processed_df, pois_df = loader.preprocess_data(raw_df)
    clustering_data = loader.get_pois_for_clustering(pois_df)

    print(f"\nç”¨äºèšç±»çš„POIæ•°æ®:")
    print(f"- POIæ•°é‡: {len(clustering_data)}")
    print(f"- ç»çº¬åº¦èŒƒå›´: {clustering_data['latitude'].min():.3f}~{clustering_data['latitude'].max():.3f}, "
          f"{clustering_data['longitude'].min():.3f}~{clustering_data['longitude'].max():.3f}")

    # 3. èšç±»è¯„ä¼° - ä½¿ç”¨ä¼˜åŒ–å‚æ•°
    evaluator = ClusteringEvaluator(clustering_data)

    # å‚æ•°ï¼ˆä½¿ç”¨kmå•ä½ï¼‰
    eps_km = 1.0  # 1.0å…¬é‡Œï¼ˆåŸæ¥0.01Â°â‰ˆ1.1kmï¼‰
    min_samples = 3
    geohash_precision = 5

    print(f"\nä½¿ç”¨å‚æ•°: eps={eps_km}, min_samples={min_samples}, geohash_precision={geohash_precision}")

    results, labels_orig, labels_geo = evaluator.compare_methods(
        eps_km=eps_km,  # æ”¹ä¸ºeps_km
        min_samples=min_samples,
        geohash_precision=geohash_precision
    )

    # 4. æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 20+"èšç±»è´¨é‡è¯„ä¼°ç»“æœ"+"=" * 20)
    display_columns = ['method', 'silhouette_score', 'calinski_harabasz_score',
                       'davies_bouldin_score', 'n_clusters', 'n_noise', 'coverage', 'avg_cluster_size']
    print(results[display_columns].round(4))

    # 5. ç»“æœåˆ†æ
    print("\n" + "=" * 20 + "æ”¹è¿›æ•ˆæœ" + "=" * 20)

    if not np.isnan(results.iloc[0]['silhouette_score']):
        sil_improve = (results.iloc[1]['silhouette_score'] - results.iloc[0]['silhouette_score']) / results.iloc[0][
            'silhouette_score'] * 100
        ch_improve = (results.iloc[1]['calinski_harabasz_score'] - results.iloc[0]['calinski_harabasz_score']) / \
                     results.iloc[0]['calinski_harabasz_score'] * 100
        db_improve = (results.iloc[0]['davies_bouldin_score'] - results.iloc[1]['davies_bouldin_score']) / \
                     results.iloc[0]['davies_bouldin_score'] * 100

        print(f"Silhouette Score å˜åŒ–: {sil_improve:+.2f}%")
        print(f"Calinski-Harabasz Score å˜åŒ–: {ch_improve:+.2f}%")
        print(f"Davies-Bouldin Score å˜åŒ–: {db_improve:+.2f}% (è¶Šä½è¶Šå¥½)")
        print(f"èšç±»æ•°é‡å˜åŒ–: {results.iloc[0]['n_clusters']} â†’ {results.iloc[1]['n_clusters']}")
        print(
            f"å™ªå£°ç‚¹æ¯”ä¾‹å˜åŒ–: {results.iloc[0]['n_noise'] / len(clustering_data):.3f} â†’ {results.iloc[1]['n_noise'] / len(clustering_data):.3f}")
        print(f"å¹³å‡ç°‡å¤§å°å˜åŒ–: {results.iloc[0]['avg_cluster_size']:.1f} â†’ {results.iloc[1]['avg_cluster_size']:.1f}")

    # 6. å¯è§†åŒ–
    save_path = f"./clustering_eps={eps_km}_minS={min_samples}_NYC_P{geohash_precision}.png"
    evaluator.visualize_comparison(labels_orig, labels_geo, save_path=save_path)


if __name__ == "__main__":
    main()